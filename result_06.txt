#完成品　動作します
import numpy as np
import os
import numpy as np
from keras.models import Sequential, Model
from keras.layers import Dense, Activation, Input, Conv2D, Flatten
from keras.engine.topology import Layer
from keras.optimizers import RMSprop
from keras.utils import to_categorical
from agvsimulator_04 import *

#n_branch=7
n_branch=4
n_actions = 2**n_branch

def build_mlp():
    model = Sequential()
    model.add(Dense(100, activation='relu', input_shape=(425,)))
    model.add(Dense(n_actions))
    model.compile(RMSprop(), 'mse')
    return model

class ReplayMemory:
    def __init__(self, memory_size):
        self.memory_size = memory_size
        self.memory = []

    def __len__(self):
        return len(self.memory)

    def append(self, transition):
        self.memory.append(transition)
        self.memory = self.memory[-self.memory_size:]

    def sample(self, batch_size):
        batch_indexes = np.random.randint(0, len(self.memory), size=batch_size).tolist()
        state      = np.array([self.memory[index]['state'] for index in batch_indexes])
        next_state = np.array([self.memory[index]['next_state'] for index in batch_indexes])
        reward     = np.array([self.memory[index]['reward'] for index in batch_indexes])
        action     = np.array([self.memory[index]['action'] for index in batch_indexes])
        terminal   = np.array([self.memory[index]['terminal'] for index in batch_indexes])
        return {'state': state, 'next_state': next_state, 'reward': reward, 'action': action, 'terminal': terminal}


def copy_weights(model_original, model_target):
    for i, layer in enumerate(model_original.layers):
        model_target.layers[i].set_weights(layer.get_weights())
    return model_target



#学習の初めはReplay Memoryが空で思い出すものが何もないため, 
#学習を始める前にランダムに行動した履歴をメモリに事前にためておきます.
memory_size = 10**6 # 10**6
initial_memory_size = 1000 # 50000

env = AGVSimulator()
replay_memory = ReplayMemory(memory_size)

step = 0    
n_steps = 1000
while True:
    state = env.reset().flatten()
    terminal = False

    while not terminal:
        action = np.random.randint(0, n_actions)
        next_state, reward, terminal, _ = env.step([int(i) for i in ('00000000'+format(action,'b'))[-n_branch:]])
        next_state = np.clip(next_state.flatten(),0,1)
        #next_state = next_state.flatten()
        reward = np.sign(reward)
        transition = {
            'state': state,
            'next_state': next_state,
            'reward': reward,
            'action': action,
            'terminal': int(terminal)
        }
        replay_memory.append(transition)
        state = next_state
        step += 1
        if (step + 1) % 10000 == 0:
            print('Number of frames:', step + 1)
        if step > n_steps:
            terminal = 1
    if step >= initial_memory_size:
        break




#学習
#ネットワークの構築
model = build_mlp()
model_target = build_mlp()
eps_start = 1.0
eps_end = 0.2

gamma = 0.9
target_update_interval = 10
batch_size = 32
n_episodes = 3000

def get_eps(step):
    return max(eps_end, (eps_end - eps_start)/n_steps * step + eps_start)

def create_target(y, _t, action, n_actions):
    one_hot = to_categorical(action, n_actions)
    t = (1 - one_hot) * y + one_hot * _t[:, None]
    return t

def train(batch_size):
    batch = replay_memory.sample(batch_size)
    try:
        q = model.predict(batch['state'])
    except:
        from IPython.core.debugger import Pdb; Pdb().set_trace()
    q_target_next = model_target.predict(batch['next_state'])
    _t = batch['reward'] + (1 - batch['terminal']) * gamma * q_target_next.max(1)
    t = create_target(q, _t, batch['action'], n_actions)
    return model.fit(batch['state'], t, epochs=1, verbose=0)

def test():
    state = env.reset()
    terminal = False
    total_reward = 0
    step = 0
    while not terminal:
        #env.render()
        q = model.predict(state.flatten()[None]).flatten()
        action = np.argmax(q)
        #print(env.state, action)
        next_state, reward, terminal, _ = env.step([int(i) for i in ('00000000'+format(action,'b'))[-n_branch:]])
        next_state = np.clip(next_state.flatten(),0,1)
        #next_state = next_state.flatten()
        total_reward += reward
        state = next_state
        step += 1
        if step > 1000:
            terminal = 1
    print('total_reward: ',total_reward,' total_step: ',step)


for episode in range(n_episodes):
    state = env.reset().flatten()
    terminal = False
    total_reward = 0
    total_q_max = []
    step = 0
    while not terminal:
        q = model.predict(state.flatten()[None]).flatten()
        total_q_max.append(np.max(q))
        eps = get_eps(step)
        if np.random.random() < eps:
            action = np.random.randint(0, n_actions)
        else:
            action = np.argmax(q)
        next_state, reward, terminal, _ = env.step([int(i) for i in ('00000000'+format(action,'b'))[-n_branch:]])
        next_state = np.clip(next_state.flatten(),0,1)
        #next_state = next_state.flatten()
        #reward = np.sign(reward)
        total_reward += reward
        #sprint(episode,step,total_reward,reward)
        transition = {
            'state': state,
            'next_state': next_state,
            'reward': reward,
            'action': action,
            'terminal': int(terminal)
        }
        replay_memory.append(transition)
        train(batch_size)
        state = next_state
        if (step + 1) % target_update_interval == 0:
            model_target = copy_weights(model, model_target)
        if step > n_steps:
            terminal = 1
        step += 1
#        print(step,'step')
    if (episode + 1) % 1 == 0:
        print('Episode: {}, Reward: {}, Q_max: {:.4f}, eps: {:.4f}, total_step: {}'.format(episode + 1, total_reward, np.mean(total_q_max), eps, step))
        print(q)
        test()
        

    
import numpy as np
import networkx as nx
#import matplotlib.pyplot as plt
#import matplotlib.cm as cm
#import matplotlib.animation as animation
from graphviz import Digraph

def onehot(n, i):
    return np.eye(n,dtype=int)[i]

def close_merge(A, edges):
    for i, j in edges:
        A[i, j] = 0
        A[i, i] = 1

def close_branch(A, edges):
    for i,j in edges:
        A[i, j] = 0

def pick_cargo(state):
    new_state = []
    for i,s in enumerate(state.T):
        if (s[0]>0) and np.all(s[1:3]==0) and np.any(s[3:]>0):
            ns = [0] + list(s[3:]) + list(s[3:]*0)
        else:
            ns = list(s)
        new_state.append(ns)
    return np.array(new_state, dtype=int).T


px = np.array([ 0 , 1, 2, 3, 4, 5, 6, 7, 8, 9,\
                10,11,12,13,14,14,14,14,14,14,\
                14,14,14,14,14,14,14,14,14,13,\
                12,11,10, 9, 8, 7, 6, 5, 4, 3,\
                2 , 1, 0, 0, 0, 0, 0, 0, 0, 0,\
                0 , 0, 0, 0, 0, 0, 1, 2, 3, 4,\
                4 ,13,12,11,10, 9, 8, 7, 6, 5,\
                4 , 3, 2, 1,13,12,11,11,11,11,\
                9 , 9, 9, 9, 9, 5, 5, 5, 5, 5,\
                11,11,11,11,11,11,11,12,13])
py = np.array([ 0 , 0, 0, 0, 0, 0, 0, 0, 0, 0,\
                0 , 0, 0, 0, 0, 1, 2, 3, 4, 5,\
                6 , 7, 8, 9,10,11,12,13,14,14,\
                14,14,14,14,14,14,14,14,14,14,\
                14,14,14,13,12,11,10, 9, 8, 7,\
                6 , 5, 4, 3, 2, 1, 2, 2, 2, 2,\
                1 , 8, 8, 8, 8, 8, 8, 8, 8, 8,\
                8 , 8, 8, 8,10,10,10,11,12,13,\
                13,12,11,10, 9, 9,10,11,12,13,\
                7 , 6, 5, 4, 3, 2, 1, 5, 5])

start_nodes=[ 0, 59]
goal_nodes =[76, 28]

class AGVSimulator(object):
    def __init__(self):
        self.n_agvs  = 5
        self.t_cargo = 2
        self.setup_network()

    def setup_network(self):
        G = nx.DiGraph()
        G.add_path(       list(range( 0,56)) + [ 0] )
        G.add_path([54] + list(range(56,61)) + [ 4])
        G.add_path([22] + list(range(61,74)) + [48])
        G.add_path([24] + list(range(74,80)) + [31])
        G.add_path([33] + list(range(80,85)) + [65])
#        G.add_path([69] + list(range(85,90)) + [37])
#        G.add_path([63] + list(range(90,97)) + [11])
#        G.add_path([92] + list(range(97,99)) + [19])
        self.G = G
        N = G.number_of_nodes()
        self.n_nodes = N
        self.A = np.array(nx.adjacency_matrix(G).todense())
        branchs = [[[i,j] for j in list(v)] \
                   for i,v in G.adj._atlas.items()  if len(v)>1]
        merges  = [[[j,i] for j in list(v)] \
                   for i,v in G.pred._atlas.items() if len(v)>1]
        self.branchs   = np.array(branchs)
        self.n_branchs = len(branchs)
        self.merges    = np.array(merges)
        self.n_merges  = len(merges)
        self.n_actions = np.array([len(b) for b in (branchs)])

    def reset(self, seed=None):
        na = self.n_agvs
        nn = self.n_nodes
        tc = self.t_cargo
        is_agv = np.zeros(nn ,dtype=int)
        is_agv[:na] = 1
        np.random.shuffle(is_agv)
        agv_state  = np.zeros([tc, nn],dtype=int)
        road_state = np.zeros([tc, nn],dtype=int)
        self.state = np.vstack([is_agv, agv_state, road_state])
        return self.state

    def animation(self,n_step=300):
        colors=np.array(['bx','ro','go'])
        fig = plt.figure()
        fig.patch.set_facecolor('black')
        ims = []
        for i in range(n_step):
            action = np.random.randint(2,size=len(self.n_actions))
            self.step(action)
            s = np.clip(self.state,0,1)
            nodes = (np.argmax(s, axis=0)+1)*np.sum(s,axis=0)
            ax = plt.gca()
            ax.patch.set_facecolor('black')
            im = ax.scatter(px, py, c=nodes, marker='s',cmap=cm.hsv, vmax=6)
            ims.append([im])
        ani = animation.ArtistAnimation(fig, ims, interval=50)
        plt.show()


    def step(self, action):
        now_state = self.state
        action_b = action
        action_m = np.random.randint(2, size=self.n_branchs)
        closedA  = np.copy(self.A)
        edges_b  = np.array([self.branchs[i][j] for i,j in enumerate(action_b)])
        close_branch(closedA, edges_b)
        edges_m  = np.array([self.merges[i][j] for i,j in enumerate(action_m)])
        close_merge( closedA, edges_m)
        is_fill_agv = np.max(self.state[:self.t_cargo+1], axis=0)>0
        is_stop = np.dot(is_fill_agv, closedA.T)
        is_stop = np.clip(1- np.clip(now_state[:self.t_cargo+1],0,1) + is_stop,0,1)
        stop_agv = now_state[:self.t_cargo+1] * is_stop
        move_agv = now_state[:self.t_cargo+1] * (1-is_stop)# stop_agv
        next_agv = np.dot(move_agv, closedA) + stop_agv
        next_state = np.vstack([next_agv, now_state[self.t_cargo+1:]])
        next_state  = pick_cargo(next_state)
        if np.all(next_state[:,start_nodes[0]] == 0):
            if np.random.rand() < 0.1:
                next_state[3,start_nodes[0]] = 1
        if np.all(next_state[:,start_nodes[1]] == 0):
            if np.random.rand() < 0.1:
                next_state[4,start_nodes[1]] = 1
        next_state = np.vstack([next_state[0],(next_state[1:]>0)*(next_state[1:]+1)])
        self.state = next_state
        reward = 0
        terminal = 0
        if self.state[1,goal_nodes[0]]>0:
#            print(self.state[1,goal_nodes[0]],'goal1')
            reward += 1 #00 - self.state[1,30]
            self.state[0,goal_nodes[0]] = 1
            self.state[1,goal_nodes[0]] = 0
            #terminal = 1
        if self.state[2,goal_nodes[1]]>0:
#            print(self.state[2,goal_nodes[1]],'goal2')
            reward += 1 #00 - self.state[2,75]
            self.state[0,goal_nodes[1]] = 1
            self.state[2,goal_nodes[1]] = 0


            #terminal = 1
#        if self.state[2,23]>0:
#            termianl = 1
#        if self.state[1,70]>0:
#            terminal = 1

#        print(self.state)]
#        print(np.max(self.state[1:]),'max')
#print(self.state)
        if np.max(self.state[1:]) > 100:
            terminal = 1
        return next_state, reward, terminal, 1


if __name__ == '__main__':
    env = AGVSimulator()
    state = env.reset()


